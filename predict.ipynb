{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from patsy import bs, dmatrix\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from patsy import dmatrix\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, DotProduct, RBF\n",
    "from pygam import LinearGAM, s, f, te\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install keras\n",
    "# %pip install tensorflow\n",
    "# %pip install scikeras\n",
    "# %pip install scikit-learn\n",
    "# %pip install pygam\n",
    "# %pip install catboost\n",
    "# %pip install lightgbm\n",
    "# %pip install xgboost\n",
    "# %pip install statsmodels\n",
    "# %pip install patsy\n",
    "# %pip install pptx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "global map_name\n",
    "map_name = 'map2' ## map2 = talhão 2. map1 = talhão 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compatibilidade com sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RBFFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, gamma=0.1):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.exp(-(X ** 2) / (2.0 * (self.gamma ** 2)))\n",
    "\n",
    "def local_regression(X, y):\n",
    "    smooth = lowess(y, X, frac=0.1)\n",
    "    return smooth[:, 1]\n",
    "\n",
    "class LocalRegressionTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([local_regression(X[:, i], X[:, -1]) for i in range(X.shape[1] - 1)]).T\n",
    "\n",
    "\n",
    "class LinearGAMWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = LinearGAM(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file into a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Loaded DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    data_frame = pd.read_csv(file_path)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separar X e Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_lat_and_long(df):\n",
    "    \"\"\"\n",
    "    Exclude latitude and longitude columns from the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Processed DataFrame without latitude and longitude columns.\n",
    "    \"\"\"\n",
    "    return df.drop(columns=['latitude', 'longitude'])\n",
    "\n",
    "\n",
    "def filter_variables_by_name(df, exclude_coordinates=True):\n",
    "    \"\"\"\n",
    "    Filter variables in the DataFrame based on the presence of '.tiff' in column names.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to be processed.\n",
    "    - exclude_coordinates: Boolean indicating whether to exclude latitude and longitude columns.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of DataFrames: One containing '.tiff' variables and the other containing non-'.tiff' variables.\n",
    "    \"\"\"\n",
    "    tiff_variables = [col for col in df.columns if '.tiff' in col]\n",
    "    non_tiff_variables = [col for col in df.columns if '.tiff' not in col]\n",
    "\n",
    "    tiff_df = df[tiff_variables]\n",
    "    non_tiff_df = df[non_tiff_variables]\n",
    "\n",
    "    if exclude_coordinates:\n",
    "        non_tiff_df = exclude_lat_and_long(non_tiff_df)\n",
    "\n",
    "    return tiff_df, non_tiff_df\n",
    "\n",
    "\n",
    "def split_target(data, target_feature):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into feature (X) and target (y) variables.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame to be split.\n",
    "    - target_feature: Name of the target feature.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of DataFrames: Feature variables (X) and target variable (y).\n",
    "    \"\"\"\n",
    "    X = data.drop(columns=[target_feature])\n",
    "    y = data[target_feature]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(X, y, coords, split_method='slice', part_to_split=10):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets based on the specified split method.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature variables.\n",
    "    - y: Target variable.\n",
    "    - coords: DataFrame containing latitude and longitude columns.\n",
    "    - split_method: Method for data splitting ('quadrant', 'slice', or 'random').\n",
    "    - part_to_split: Number of parts to split the data.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of DataFrames: Training and testing sets for feature variables and target variable.\n",
    "    \"\"\"\n",
    "    if split_method == 'quadrant':\n",
    "        return split_data_by_quadrant(X, y, coords, part_to_split)\n",
    "    elif split_method == 'random':\n",
    "        return split_data_randomly(X, y)\n",
    "    elif split_method == 'slice':\n",
    "        return split_data_by_slice(X, y, coords, part_to_split)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split method. Valid options are 'quadrant', 'slice', and 'random'.\")\n",
    "\n",
    "\n",
    "def divide_into_quadrants(df, lat_col, lon_col):\n",
    "    \"\"\"\n",
    "    Divide the DataFrame into four quadrants based on the mean coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to be divided.\n",
    "    - lat_col: Name of the latitude column.\n",
    "    - lon_col: Name of the longitude column.\n",
    "\n",
    "    Returns:\n",
    "    - List of DataFrames: Four quadrants of the original DataFrame.\n",
    "    \"\"\"\n",
    "    # Calculate the average of coordinates to find the central point\n",
    "    lat_mean = df[lat_col].mean()\n",
    "    lon_mean = df[lon_col].mean()\n",
    "\n",
    "    # Divide the DataFrame into four quadrants\n",
    "    quadrant_1 = df[(df[lat_col] >= lat_mean) & (df[lon_col] >= lon_mean)]\n",
    "    quadrant_2 = df[(df[lat_col] >= lat_mean) & (df[lon_col] < lon_mean)]\n",
    "    quadrant_3 = df[(df[lat_col] < lat_mean) & (df[lon_col] >= lon_mean)]\n",
    "    quadrant_4 = df[(df[lat_col] < lat_mean) & (df[lon_col] < lon_mean)]\n",
    "\n",
    "    quadrant_list = [quadrant_1, quadrant_2, quadrant_3, quadrant_4]\n",
    "    # Return the three quadrants together and the separated quadrant\n",
    "    return quadrant_list\n",
    "\n",
    "\n",
    "def divide_into_slices(df, lat_col, lon_col, N):\n",
    "    \"\"\"\n",
    "    Divide the DataFrame into N slices based on latitude and longitude columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to be divided.\n",
    "    - lat_col: Name of the latitude column.\n",
    "    - lon_col: Name of the longitude column.\n",
    "    - N: Number of slices.\n",
    "\n",
    "    Returns:\n",
    "    - List of DataFrames: N slices of the original DataFrame.\n",
    "    \"\"\"\n",
    "    # Check if N is a valid value\n",
    "    if N <= 0:\n",
    "        raise ValueError(\"N must be a positive integer.\")\n",
    "\n",
    "    # Sort the DataFrame based on latitude and longitude columns\n",
    "    df_sorted = df.sort_values(by=[lat_col, lon_col])\n",
    "\n",
    "    # Calculate the number of samples in each part\n",
    "    num_samples = len(df_sorted)\n",
    "    samples_per_part = num_samples // N\n",
    "    remainder = num_samples % N\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    # Divide the DataFrame into N parts\n",
    "    start_idx = 0\n",
    "    for i in range(N):\n",
    "        end_idx = start_idx + samples_per_part + (1 if i < remainder else 0)\n",
    "        current_part = df_sorted.iloc[start_idx:end_idx]\n",
    "        parts.append(current_part)\n",
    "        start_idx = end_idx\n",
    "\n",
    "    return parts\n",
    "\n",
    "\n",
    "def merge_df(X, y):\n",
    "    \"\"\"\n",
    "    Merge feature variables (X) and target variable (y) into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature variables.\n",
    "    - y: Target variable.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Merged DataFrame containing both X and y.\n",
    "    \"\"\"\n",
    "    return pd.concat([X, y], axis=1)\n",
    "\n",
    "\n",
    "def split_data_by_slice(X, y, coords, slice_to_split=10):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets by dividing into slices.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature variables.\n",
    "    - y: Target variable.\n",
    "    - coords: DataFrame containing latitude and longitude columns.\n",
    "    - slice_to_split: Index of the slice to be used for testing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of DataFrames: Training and testing sets for feature variables and target variable.\n",
    "    \"\"\"\n",
    "    # Merge X and y\n",
    "    new_data = merge_df(X, y)\n",
    "\n",
    "    # Merge X and y with coordinates\n",
    "    new_data = merge_df(new_data, coords)\n",
    "\n",
    "    # Divide into slices\n",
    "    slice_list = divide_into_slices(new_data, 'latitude', 'longitude', 10)\n",
    "\n",
    "    # Get the slice to be used for testing\n",
    "    test_slice = slice_list.pop(slice_to_split - 1)\n",
    "\n",
    "    # Combine the other slices\n",
    "    train_slices = combine_parts(slice_list)\n",
    "\n",
    "    # Separate band and nutrient variables\n",
    "    train_slice_band, train_slice_nutrients = filter_variables_by_name(train_slices)\n",
    "    test_slice_band, test_slice_nutrients = filter_variables_by_name(test_slice)\n",
    "\n",
    "    return train_slice_band, test_slice_band, train_slice_nutrients, test_slice_nutrients\n",
    "\n",
    "\n",
    "def split_data_by_quadrant(X, y, coords, quadrant_to_split=4):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets by dividing into quadrants.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature variables.\n",
    "    - y: Target variable.\n",
    "    - coords: DataFrame containing latitude and longitude columns.\n",
    "    - quadrant_to_split: Index of the quadrant to be used for testing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of DataFrames: Training and testing sets for feature variables and target variable.\n",
    "    \"\"\"\n",
    "    # Merge X and y\n",
    "    new_data = merge_df(X, y)\n",
    "\n",
    "    # Merge X and y with coordinates\n",
    "    new_data = merge_df(new_data, coords)\n",
    "\n",
    "    # Divide into quadrants\n",
    "    quadrant_list = divide_into_quadrants(new_data, 'latitude', 'longitude')\n",
    "\n",
    "    # Get the quadrant to be used for testing\n",
    "    test_quadrant = quadrant_list.pop(quadrant_to_split - 1)\n",
    "\n",
    "    # Combine the other quadrants\n",
    "    train_quadrants = combine_quadrants(quadrant_list)\n",
    "\n",
    "    # Separate band and nutrient variables\n",
    "    train_quadrant_band, train_quadrant_nutrients = filter_variables_by_name(train_quadrants)\n",
    "    test_quadrant_band, test_quadrant_nutrients = filter_variables_by_name(test_quadrant)\n",
    "\n",
    "    return train_quadrant_band, test_quadrant_band, train_quadrant_nutrients, test_quadrant_nutrients\n",
    "\n",
    "\n",
    "def split_data_randomly(X, y, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets randomly.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature variables.\n",
    "    - y: Target variable.\n",
    "    - test_size: Proportion of the data to include in the test split.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of DataFrames: Training and testing sets for feature variables and target variable.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def combine_parts(part_list):\n",
    "    \"\"\"\n",
    "    Combine multiple DataFrames into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - part_list: List of DataFrames to be combined.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Combined DataFrame.\n",
    "    \"\"\"\n",
    "    # Check if at least one DataFrame is provided\n",
    "    if not part_list or len(part_list) < 2:\n",
    "        raise ValueError(\"At least two DataFrames are required for combination.\")\n",
    "\n",
    "    # Combine all DataFrames in the list into a single DataFrame\n",
    "    combined_df = pd.concat(part_list, ignore_index=True)\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(numerical_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Create a data preprocessor using a column transformer with separate pipelines for numerical and categorical features.\n",
    "\n",
    "    Parameters:\n",
    "    - numerical_features: List of numerical feature names.\n",
    "    - categorical_features: List of categorical feature names.\n",
    "\n",
    "    Returns:\n",
    "    - preprocessor: ColumnTransformer for data preprocessing.\n",
    "    \"\"\"\n",
    "    ## Handling categorical variables\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('encoder', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    ## Handling numerical variables\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    ## Transforming the data\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('numbers', numerical_transformer, numerical_features),\n",
    "        ('categories', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "    ## Returning the data processor\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.metrics import MeanAbsolutePercentageError, RootMeanSquaredError, MeanSquaredError, MeanAbsoluteError\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import MeanAbsoluteError\n",
    "\n",
    "\n",
    "def get_hidden_layers(input_neurons, output_neurons):\n",
    "    \"\"\"\n",
    "    Get the number of hidden layers for a neural network based on the number of input and output neurons.\n",
    "\n",
    "    Parameters:\n",
    "    - input_neurons: Number of input neurons.\n",
    "    - output_neurons: Number of output neurons.\n",
    "\n",
    "    Returns:\n",
    "    - int: Number of hidden layers.\n",
    "    \"\"\"\n",
    "    # Calculate the number of hidden layers based on the number of input and output neurons\n",
    "    hidden_layers = int(np.log2(input_neurons * output_neurons))\n",
    "\n",
    "    return hidden_layers\n",
    "\n",
    "# Função para construir o modelo de rede neural\n",
    "# def build_custom_neural_network(optimizer='adam', loss='mse', learning_rate=0.001, input_neurons=None, output_neurons=None, num_hidden_layers=1, neurons_per_layer=None):\n",
    "#     if optimizer == 'adam':\n",
    "#         optimizer = Adam(learning_rate=learning_rate)\n",
    "#     elif optimizer == 'sgd':\n",
    "#         optimizer = SGD(learning_rate=learning_rate)\n",
    "#     elif optimizer == 'rmsprop':\n",
    "#         optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "#     model = Sequential()\n",
    "    \n",
    "#     # Adicionando a camada de entrada\n",
    "#     model.add(Dense(input_neurons, input_dim=input_neurons, activation='relu'))\n",
    "    \n",
    "#     # Adicionando camadas ocultas\n",
    "#     if neurons_per_layer is None:\n",
    "#         neurons_per_layer = [input_neurons // (2 ** i) for i in range(num_hidden_layers)]\n",
    "    \n",
    "#     for neurons in neurons_per_layer:\n",
    "#         model.add(Dense(neurons, activation='relu'))\n",
    "    \n",
    "#     # Adicionando a camada de saída\n",
    "#     model.add(Dense(output_neurons, activation='linear'))  # Utilize 'linear' se for um problema de regressão\n",
    "    \n",
    "#     # Lista de métricas adicionais\n",
    "#     metrics = [MeanAbsolutePercentageError(), RootMeanSquaredError(), MeanSquaredError(), MeanAbsoluteError()]\n",
    "    \n",
    "#     # Compilando o modelo com métricas adicionais\n",
    "#     model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "def build_simple_neural_network(input_neurons, output_neurons, optimizer='adam', loss='mse'):\n",
    "    \"\"\"\n",
    "    Build a simple neural network model with one hidden layer.\n",
    "\n",
    "    Parameters:\n",
    "    - input_neurons: Number of input neurons.\n",
    "    - output_neurons: Number of output neurons.\n",
    "    - optimizer: Optimizer for the model.\n",
    "    - loss: Loss function for the model.\n",
    "    \n",
    "    Returns:\n",
    "    - Sequential: Compiled neural network model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adicionando a camada de entrada\n",
    "    model.add(Dense(input_neurons, input_dim=input_neurons, activation='relu'))\n",
    "    \n",
    "    # Adicionando a camada oculta com o mesmo número de neurônios da camada de entrada\n",
    "    model.add(Dense(input_neurons, activation='relu'))\n",
    "    \n",
    "    # Adicionando a camada de saída\n",
    "    model.add(Dense(output_neurons, activation='linear'))  # Utilize 'linear' se for um problema de regressão\n",
    "    \n",
    "    # Compilando o modelo\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[MeanSquaredError()])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def create_models_pipeline(preprocessor, grid, neural_network = False, input_layer_neurons=1, output_layer_neurons=1, n_folds=10):\n",
    "    \"\"\"\n",
    "    Create a pipeline of models, with or without hyperparameter tuning using grid search.\n",
    "\n",
    "    Parameters:\n",
    "    - preprocessor: Data preprocessor for feature transformation.\n",
    "    - grid: Boolean indicating whether to use grid search for hyperparameter tuning.\n",
    "    - n_folds: Number of folds for cross-validation.\n",
    "    - neural_network: Boolean indicating whether to include a neural network model.\n",
    "\n",
    "    Returns:\n",
    "    - models_pipeline: Dictionary containing models as values, with or without grid search.\n",
    "    \"\"\"\n",
    "    models_pipeline = create_models(preprocessor, neural_network, input_layer_neurons, output_layer_neurons)\n",
    "\n",
    "    if grid:\n",
    "        grid_models_pipeline = create_grid_models(models_pipeline, n_folds, input_layer_neurons, output_layer_neurons)\n",
    "        return grid_models_pipeline\n",
    "\n",
    "    return models_pipeline\n",
    "\n",
    "\n",
    "def create_models(preprocessor, neural_network = False, input_layer_neurons = 1, output_layer_neurons = 1):\n",
    "    \"\"\"\n",
    "    Create a dictionary of models with associated pipelines for training.\n",
    "\n",
    "    Parameters:\n",
    "    - preprocessor: Data preprocessor for feature transformation.\n",
    "    - neural_network: Boolean indicating whether to include a neural network model.\n",
    "    - input_layer_neurons: Number of input neurons for the neural network model.\n",
    "    - output_layer_neurons: Number of output neurons for the neural network model.\n",
    "\n",
    "    Returns:\n",
    "    - models: Dictionary containing machine learning models with associated pipelines.\n",
    "    \"\"\"\n",
    "\n",
    "    if neural_network:\n",
    "        models = {\n",
    "            'NeuralNetwork': KerasRegressor(\n",
    "                build_fn=build_simple_neural_network,\n",
    "                input_neurons=input_layer_neurons,\n",
    "                output_neurons=output_layer_neurons,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                verbose=0\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # Create pipelines for each model\n",
    "        for model_name, model in models.items():\n",
    "            models[model_name] = Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('dim_red', PCA(5)),\n",
    "                ('model', model)\n",
    "            ])\n",
    "\n",
    "        return models\n",
    "        \n",
    "    else:\n",
    "        models = {\n",
    "            'LinearRegression': LinearRegression(),\n",
    "            # 'RandomForest': RandomForestRegressor(),\n",
    "            # 'SVR': SVR(),\n",
    "            # 'Ridge': Ridge(),\n",
    "            # 'Lasso': Lasso(),\n",
    "            # 'CatBoost': CatBoostRegressor(),\n",
    "            # 'LGBM': LGBMRegressor(),\n",
    "            # 'XGBoost': XGBRegressor(),\n",
    "            # 'BayesianRidge': BayesianRidge(),\n",
    "            # 'KNeighbors': KNeighborsRegressor(),\n",
    "            # 'GradientBoosting': GradientBoostingRegressor(),\n",
    "            # 'MLP': MLPRegressor(),\n",
    "            # 'ElasticNet': ElasticNet(),\n",
    "            # 'HuberRegressor': HuberRegressor(),\n",
    "            # 'GAM': LinearGAMWrapper(),\n",
    "        }\n",
    "\n",
    "        # Create pipelines for each model\n",
    "        for model_name, model in models.items():\n",
    "            models[model_name] = Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('dim_red', PCA()),\n",
    "                ('model', model)\n",
    "            ])\n",
    "\n",
    "        # Special case for Polynomial Regression\n",
    "        # models['PolynomialRegression'] = Pipeline(steps=[\n",
    "        #     ('preprocessor', preprocessor),\n",
    "        #     ('poly', PolynomialFeatures(degree=2)),\n",
    "        #     ('dim_red', PCA()),\n",
    "        #     ('model', LinearRegression())\n",
    "        # ])\n",
    "        \n",
    "        \n",
    "        # models['BasisFunctions'] = Pipeline(steps=[\n",
    "        #     ('preprocessor', preprocessor),\n",
    "        #     ('basis', RBFFeatureTransformer()),\n",
    "        #     ('dim_red', PCA()),\n",
    "        #     ('model', LinearRegression())\n",
    "        # ])\n",
    "\n",
    "        # models['LocalRegression'] = Pipeline(steps=[\n",
    "        #     ('preprocessor', preprocessor),\n",
    "        #     ('local', LocalRegressionTransformer()),\n",
    "        #     ('dim_red', PCA()),\n",
    "        #     ('model', LinearRegression())\n",
    "        # ])\n",
    "\n",
    "        return models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_models(models_pipeline, n_folds = 10 , input_layer_neurons = 1, output_layer_neurons = 1):\n",
    "    \"\"\"\n",
    "    Create a dictionary of GridSearchCV objects with hyperparameter grids for each model.\n",
    "\n",
    "    Parameters:\n",
    "    - models_pipeline: Dictionary containing machine learning models to be tuned.\n",
    "    - n_folds: Number of folds for cross-validation.\n",
    "    - input_layer_neurons: Number of input neurons for the neural network model.\n",
    "    - output_layer_neurons: Number of output neurons for the neural network model.\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    - grid_models: Dictionary containing GridSearchCV objects for each model.\n",
    "    \"\"\"\n",
    "\n",
    "    grid_models = {}\n",
    "\n",
    "    grid_params = {\n",
    "        'LinearRegression': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__fit_intercept': [True, False],\n",
    "            # 'model__normalize': [True, False],\n",
    "            # 'model__copy_X': [True, False],\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__n_estimators': [100, 200, 300, 1000],\n",
    "            # 'model__max_depth': [None, 5, 10, 20],\n",
    "            # 'model__min_samples_split': [2, 5, 10],\n",
    "            # 'model__min_samples_leaf': [1, 2, 4],\n",
    "            # 'model__max_features': ['auto', 'sqrt', 'log2'],\n",
    "            # 'model__bootstrap': [True, False],\n",
    "        },\n",
    "        'SVR': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            # 'model__degree': [3, 5, 7],\n",
    "            # 'model__gamma': ['scale', 'auto'],\n",
    "            # 'model__coef0': [0.0, 1.0, 2.0],\n",
    "            # 'model__tol': [0.001, 0.01, 0.1],\n",
    "            # 'model__C': [1.0, 2.0, 3.0],\n",
    "            # 'model__epsilon': [0.1, 0.2, 0.3],\n",
    "            # 'model__shrinking': [True, False],\n",
    "            # 'model__cache_size': [200, 500, 1000],\n",
    "            # 'model__verbose': [True, False],\n",
    "            # 'model__max_iter': [-1],\n",
    "        },\n",
    "        'GAM': {\n",
    "            'dim_red__n_components': [5],\n",
    "        #     'model__terms': [[s(0), s(1), s(2)]],\n",
    "        #     'model__n_splines': [10, 20, 30],\n",
    "        #     'model__lam': [[0.6], [0.8], [1.0]],  # Ajustado para uma lista aninhada\n",
    "        #     'model__tol': [0.001, 0.01, 0.1],\n",
    "        #     'model__max_iter': [100, 200, 300],\n",
    "        #     'model__scale': [None],        \n",
    "        },\n",
    "        'Ridge': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__alpha': [0.1, 1.0, 2.0],\n",
    "            # 'model__fit_intercept': [True, False],\n",
    "            # 'model__copy_X': [True, False],\n",
    "            # 'model__max_iter': [None],\n",
    "            # 'model__tol': [0.001, 0.01, 0.1],\n",
    "            # 'model__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "            # 'model__random_state': [None],\n",
    "        },\n",
    "        'Lasso': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__alpha': [0.1, 1.0, 2.0],\n",
    "            # 'model__fit_intercept': [True, False],\n",
    "            # 'model__precompute': [True, False],\n",
    "            # 'model__copy_X': [True, False],\n",
    "            # 'model__max_iter': [1000],\n",
    "            # 'model__tol': [0.0001, 0.001, 0.01],\n",
    "            # 'model__warm_start': [True, False],\n",
    "            # 'model__positive': [True, False],\n",
    "            # 'model__selection': ['cyclic', 'random'],\n",
    "            # 'model__random_state': [None],\n",
    "        },\n",
    "        'CatBoost': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__iterations': [100, 200, 300],\n",
    "            # 'model__learning_rate': [0.03, 0.1, 0.3],\n",
    "            # 'model__depth': [4, 6, 10],\n",
    "            # 'model__l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "            # 'model__border_count': [32, 5, 10, 20, 50, 100, 200],\n",
    "            # 'model__thread_count': [4],\n",
    "            # 'model__random_state': [None],\n",
    "        },\n",
    "        'LGBM': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__boosting_type': ['gbdt', 'dart', 'goss', 'rf'],\n",
    "            # 'model__num_leaves': [31, 50, 100],\n",
    "            # 'model__max_depth': [-1, 5, 10, 20],\n",
    "            # 'model__learning_rate': [0.001, 0.01, 0.1],\n",
    "            # 'model__n_estimators': [100, 200, 300],\n",
    "            # 'model__subsample_for_bin': [200000, 500000, 1000000],\n",
    "            # 'model__objective': ['regression', 'regression_l1', 'mape'],\n",
    "            # 'model__class_weight': [None],\n",
    "            # 'model__min_split_gain': [0.0],\n",
    "            # 'model__min_child_weight': [0.001],\n",
    "            # 'model__min_child_samples': [20],\n",
    "            # 'model__subsample': [1.0],\n",
    "            # 'model__subsample_freq': [0],\n",
    "            # 'model__colsample_bytree': [1.0],\n",
    "            # 'model__reg_alpha': [0.0],\n",
    "            # 'model__reg_lambda': [0.0],\n",
    "            # 'model__random_state': [None],\n",
    "            # 'model__n_jobs': [-1],\n",
    "            # 'model__silent': [True],\n",
    "            # 'model__importance_type': ['split', 'gain'],\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__max_depth': [3, 5, 10],\n",
    "            # 'model__learning_rate': [0.001, 0.01, 0.1],\n",
    "            # 'model__n_estimators': [100, 200, 300],\n",
    "            # 'model__verbosity': [0],\n",
    "            # 'model__silent': [None],\n",
    "            # 'model__objective': ['reg:linear', 'reg:gamma', 'reg:tweedie'],\n",
    "            # 'model__booster': ['gbtree', 'gblinear', 'dart'],\n",
    "            # 'model__n_jobs': [-1],\n",
    "            # 'model__gamma': [0.0],\n",
    "            # 'model__min_child_weight': [1.0],\n",
    "            # 'model__max_delta_step': [0.0],\n",
    "            # 'model__subsample': [1.0],\n",
    "            # 'model__colsample_bytree': [1.0],\n",
    "            # 'model__colsample_bylevel': [1.0],\n",
    "            # 'model__colsample_bynode': [1.0],\n",
    "            # 'model__reg_alpha': [0.0],\n",
    "            # 'model__reg_lambda': [1.0],\n",
    "            # 'model__scale_pos_weight': [1.0],\n",
    "            # 'model__base_score': [0.5],\n",
    "            # 'model__random_state': [None],\n",
    "            # 'model__missing': [None],\n",
    "            # 'model__importance_type': ['gain', 'weight', 'cover', 'total_gain', 'total_cover'],\n",
    "        },\n",
    "        'BayesianRidge': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__n_iter': [300, 500, 1000],\n",
    "            # 'model__tol': [0.001, 0.01, 0.1],\n",
    "            # 'model__alpha_1': [1e-06, 1e-05, 1e-04],\n",
    "            # 'model__alpha_2': [1e-06, 1e-05, 1e-04],\n",
    "            # 'model__lambda_1': [1e-06, 1e-05, 1e-04],\n",
    "            # 'model__lambda_2': [1e-06, 1e-05, 1e-04],\n",
    "            # 'model__compute_score': [False],\n",
    "            # 'model__fit_intercept': [True, False],\n",
    "            # 'model__normalize': [True, False],\n",
    "            # 'model__copy_X': [True, False],\n",
    "            # 'model__verbose': [False],\n",
    "        },\n",
    "        'KNeighbors': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__n_neighbors': [5, 10, 20],\n",
    "            # 'model__weights': ['uniform', 'distance'],\n",
    "            # 'model__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            # 'model__leaf_size': [30],\n",
    "            # 'model__p': [1, 2],\n",
    "            # 'model__metric': ['minkowski'],\n",
    "            # 'model__metric_params': [None],\n",
    "            # 'model__n_jobs': [None],\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__loss': ['ls', 'lad', 'huber', 'quantile'],\n",
    "            # 'model__learning_rate': [0.001, 0.01, 0.1],\n",
    "            # 'model__n_estimators': [100, 200, 300],\n",
    "            # 'model__subsample': [1.0],\n",
    "            # 'model__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "            # 'model__min_samples_split': [2, 5, 10],\n",
    "            # 'model__min_samples_leaf': [1, 2, 4],\n",
    "            # 'model__min_weight_fraction_leaf': [0.0],\n",
    "            # 'model__max_depth': [3, 5, 10],\n",
    "            # 'model__min_impurity_decrease': [0.0],\n",
    "            # 'model__min_impurity_split': [None],\n",
    "            # 'model__init': [None],\n",
    "            # 'model__random_state': [None],\n",
    "            # 'model__max_features': [None],\n",
    "            # 'model__alpha': [0.9],\n",
    "            # 'model__verbose': [0],\n",
    "            # 'model__max_leaf_nodes': [None],\n",
    "            # 'model__warm_start': [False],\n",
    "            # 'model__presort': ['deprecated'],\n",
    "            # 'model__validation_fraction': [0.1],\n",
    "            # 'model__n_iter_no_change': [None],\n",
    "            # 'model__tol': [0.0001],\n",
    "            # 'model__ccp_alpha': [0.0],\n",
    "        },\n",
    "        'MLP': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__hidden_layer_sizes': [(100,), (50, 50), (50, 50, 50)],\n",
    "            # 'model__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            # 'model__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "            # 'model__alpha': [0.0001, 0.001, 0.01],\n",
    "            # 'model__batch_size': ['auto'],\n",
    "            # 'model__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "            # 'model__learning_rate_init': [0.001],\n",
    "            # 'model__power_t': [0.5],\n",
    "            # 'model__max_iter': [200, 500, 1000],\n",
    "            # 'model__shuffle': [True],\n",
    "            # 'model__random_state': [None],\n",
    "            # 'model__tol': [0.0001],\n",
    "            # 'model__verbose': [False],\n",
    "            # 'model__warm_start': [False],\n",
    "            # 'model__momentum': [0.9],\n",
    "            # 'model__nesterovs_momentum': [True],\n",
    "            # 'model__early_stopping': [False],\n",
    "            # 'model__validation_fraction': [0.1],\n",
    "            # 'model__beta_1': [0.9],\n",
    "            # 'model__beta_2': [0.999],\n",
    "            # 'model__epsilon': [1e-08],\n",
    "            # 'model__n_iter_no_change': [10],\n",
    "            # 'model__max_fun': [15000],\n",
    "        },\n",
    "        'ElasticNet': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__alpha': [0.1, 1.0, 2.0],\n",
    "            # 'model__l1_ratio': [0.1, 0.5, 0.9],\n",
    "            # 'model__fit_intercept': [True, False],\n",
    "            # 'model__normalize': [True, False],\n",
    "            # 'model__precompute': [True, False],\n",
    "            # 'model__max_iter': [1000],\n",
    "            # 'model__copy_X': [True, False],\n",
    "            # 'model__tol': [0.0001, 0.001, 0.01],\n",
    "            # 'model__warm_start': [True, False],\n",
    "            # 'model__positive': [True, False],\n",
    "            # 'model__random_state': [None],\n",
    "            # 'model__selection': ['cyclic', 'random'],\n",
    "        },\n",
    "        'HuberRegressor': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__epsilon': [1.35, 1.5, 1.75],\n",
    "            # 'model__max_iter': [100, 200, 300],\n",
    "            # 'model__alpha': [0.0001, 0.001, 0.01],\n",
    "            # 'model__warm_start': [False],\n",
    "        },    \n",
    "        'PolynomialRegression': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'poly__degree': [2, 3, 4],\n",
    "        },\n",
    "        'SplineRegression': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__n_splines': [10, 20, 30],\n",
    "            # 'model__lam': [0.6, 0.8, 1.0],\n",
    "            # 'model__tol': [0.001, 0.01, 0.1],\n",
    "            # 'model__max_iter': [100, 200, 300],\n",
    "            # 'model__fit_intercept': [True, False],\n",
    "            # 'model__scale': [None],\n",
    "        },\n",
    "        'SmoothingSplines': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__n_splines': [10, 20, 30],\n",
    "            # 'model__lam': [0.6, 0.8, 1.0],\n",
    "            # 'model__tol': [0.001, 0.01, 0.1],\n",
    "            # 'model__max_iter': [100, 200, 300],\n",
    "            # 'model__fit_intercept': [True, False],\n",
    "            # 'model__scale': [None],\n",
    "        },\n",
    "        'StepFunctions': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__n_splines': [10, 20, 30],\n",
    "            # 'model__lam': [0.6, 0.8, 1.0],\n",
    "            # 'model__tol': [0.001, 0.01, 0.1],\n",
    "            # 'model__max_iter': [100, 200, 300],\n",
    "            # 'model__fit_intercept': [True, False],\n",
    "            # 'model__scale': [None],\n",
    "        },\n",
    "        'BasisFunctions': {\n",
    "            'dim_red__n_components': [5],\n",
    "            # 'model__gamma': [0.1, 0.5, 1.0],\n",
    "        },\n",
    "        'LocalRegression': {\n",
    "            'dim_red__n_components': [5],\n",
    "        },\n",
    "        'NeuralNetwork': {\n",
    "            'model__optimizer': ['adam'],\n",
    "            'model__input_neurons': [input_layer_neurons,],  # Formato da entrada da rede neural\n",
    "            'model__output_neurons': [output_layer_neurons],  # Formato da saída da rede neural\n",
    "            'model__epochs': [100],\n",
    "            'model__loss': ['mse'],\n",
    "            'model__batch_size': [32]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create GridSearchCV objects for each model\n",
    "    for model_name, model in models_pipeline.items():\n",
    "        grid_models[model_name] = GridSearchCV(model, param_grid = grid_params[model_name], cv = KFold(n_splits=n_folds, shuffle=False), scoring = 'neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "\n",
    "    return grid_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def log_CV_scores(model_name, grid_result):\n",
    "    \"\"\"\n",
    "    Log the best parameters and score of a model from grid search results.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: Name of the model.\n",
    "    - grid_result: Result of grid search.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    best_params = grid_result.best_params_\n",
    "    best_score = grid_result.best_score_\n",
    "\n",
    "    print(f'Best parameters of {model_name} are: {best_params}')\n",
    "    print(f'Best score of {model_name} is: {best_score}\\n')\n",
    "\n",
    "    mean_cv_score = grid_result.cv_results_['mean_test_score'][0]\n",
    "    std_cv_score = grid_result.cv_results_['std_test_score'][0]\n",
    "\n",
    "    print(f'Mean CVs score: {mean_cv_score}')\n",
    "    print(f'Stds CVs score: {std_cv_score}\\n')\n",
    "\n",
    "\n",
    "def train_models(X, y, models_pipeline, grid):\n",
    "    \"\"\"\n",
    "    Train machine learning models using a specified pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature matrix of the dataset.\n",
    "    - y: Target variable of the dataset.\n",
    "    - models_pipeline: Dictionary containing machine learning models to be trained.\n",
    "    - grid: Boolean indicating whether to perform grid search.\n",
    "\n",
    "    Returns:\n",
    "    - models: Dictionary containing trained machine learning models.\n",
    "    - scores: Dictionary containing training scores for each model.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    scores = {}\n",
    "\n",
    "    # Loop through each model in the pipeline\n",
    "    for name, model in models_pipeline.items():\n",
    "        print(f'Training {name}')\n",
    "\n",
    "        # Train the model with or without grid search\n",
    "        if grid:\n",
    "            grid_result = model.fit(X, y)\n",
    "\n",
    "            # Extract and log training metrics from grid search results\n",
    "            train_metrics = {\n",
    "                'Mean MSE': abs(grid_result.cv_results_['mean_test_score'][0]),\n",
    "                'Std MSE': abs(grid_result.cv_results_['std_test_score'][0]),\n",
    "            }\n",
    "            \n",
    "            log_CV_scores(name, grid_result)\n",
    "\n",
    "            # Save the best estimator in the models dictionary\n",
    "            models[name] = grid_result.best_estimator_\n",
    "\n",
    "            # Store training metrics in the scores dictionary\n",
    "            scores[name] = train_metrics\n",
    "\n",
    "        else:\n",
    "            models[name] = model.fit(X, y)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Return the trained models and their training scores\n",
    "    return models, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_models(X, y, models, neural_network=False):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of machine learning models on a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature matrix of the dataset.\n",
    "    - y: Target variable of the dataset.\n",
    "    - models: Dictionary containing machine learning models.\n",
    "    - neural_network: Boolean indicating whether the models are neural networks.\n",
    "\n",
    "    Returns:\n",
    "    - models_scores: Dictionary containing evaluation scores for each model.\n",
    "    \"\"\"\n",
    "    models_scores = {}  # Dictionary to store evaluation scores for each model\n",
    "    scores_df = pd.DataFrame()  # Initialize an empty DataFrame for the scores\n",
    "\n",
    "    # Loop through each model\n",
    "    for name, model in models.items():\n",
    "        # Make predictions using the model\n",
    "        predictions = model.predict(X)\n",
    "\n",
    "        # Calculate evaluation scores\n",
    "        if not neural_network:\n",
    "            scores = calculate_non_neural_scores(y, predictions)\n",
    "            models_scores[name] = scores\n",
    "        else:\n",
    "            scores_df = calculate_neural_scores(y, predictions, name, scores_df)\n",
    "\n",
    "    if neural_network:\n",
    "        return scores_df\n",
    "    else:\n",
    "        return models_scores\n",
    "\n",
    "def calculate_non_neural_scores(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate evaluation scores for non-neural network models.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: True target values.\n",
    "    - y_pred: Predicted values.\n",
    "\n",
    "    Returns:\n",
    "    - scores: Dictionary containing evaluation scores.\n",
    "    \"\"\"\n",
    "    scores = {\n",
    "        'MSE': mean_squared_error(y_true, y_pred),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred)\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "def calculate_neural_scores(y_true, y_pred, name, scores_df):\n",
    "    \"\"\"\n",
    "    Calculate evaluation scores for neural network models.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: True target values.\n",
    "    - y_pred: Predicted values.\n",
    "    - name: Name of the model.\n",
    "    - scores_df: DataFrame containing evaluation scores for each model.\n",
    "\n",
    "    Returns:\n",
    "    - scores_df: Updated DataFrame containing evaluation scores for each model.\n",
    "    \"\"\"\n",
    "    model_scores = {}\n",
    "    for i, nutrient in enumerate(y_true.columns):\n",
    "        nutrient_scores = {\n",
    "            'MSE': mean_squared_error(y_true[nutrient], y_pred[:, i]),\n",
    "            'MAE': mean_absolute_error(y_true[nutrient], y_pred[:, i]),\n",
    "            'R2': r2_score(y_true[nutrient], y_pred[:, i]),\n",
    "            'MAPE': mean_absolute_percentage_error(y_true[nutrient], y_pred[:, i])\n",
    "        }\n",
    "        model_scores[nutrient] = nutrient_scores\n",
    "    \n",
    "    model_df = pd.DataFrame(model_scores).stack().unstack(0)\n",
    "    model_df.columns = pd.MultiIndex.from_product([[name], model_df.columns])\n",
    "    \n",
    "    scores_df = pd.concat([scores_df, model_df], axis=1)\n",
    "\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_results(df, data_split_method, part_to_split, grid, neural_network):\n",
    "    \"\"\"\n",
    "    Save evaluation results DataFrame to an Excel file with a timestamped filename.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing evaluation results.\n",
    "    - data_split_method: Method used for data splitting.\n",
    "    - part_to_split: Number of parts to split the data.\n",
    "    - grid: Boolean indicating whether a grid search was performed.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    filename = create_filename(data_split_method, part_to_split, grid, now, neural_network)\n",
    "\n",
    "    df.to_excel(filename)\n",
    "\n",
    "    print(f'Results saved at {filename}')\n",
    "\n",
    "def create_filename(data_split_method, part_to_split, grid, timestamp, neural_network):\n",
    "    \"\"\"\n",
    "    Create a filename based on the parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - data_split_method: Method used for data splitting.\n",
    "    - part_to_split: Number of parts to split the data.\n",
    "    - grid: Boolean indicating whether a grid search was performed.\n",
    "    - timestamp: Timestamp for the filename.\n",
    "\n",
    "    Returns:\n",
    "    - filename: Formatted filename.\n",
    "    \"\"\"\n",
    "    if neural_network:\n",
    "        filename = f'data/{map_name}/prediction_results/neural_network/neural_network_{timestamp}'\n",
    "    else:\n",
    "        filename = f'data/{map_name}/prediction_results/all_models/all_models_{timestamp}'\n",
    "\n",
    "    if data_split_method in ('quadrant', 'slice'):\n",
    "        filename += f'_part_splited_{part_to_split}'\n",
    "    filename += '_results.xlsx'\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "def create_directory_if_not_exists(directory):\n",
    "    \"\"\"\n",
    "    Create a directory if it does not exist.\n",
    "\n",
    "    Parameters:\n",
    "    - directory: Path to the directory to be created.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Check if the directory exists; if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def save_models(models, data_split_method, target_column, neural_network=False):\n",
    "    \"\"\"\n",
    "    Save trained models to pickle files with timestamped filenames.\n",
    "\n",
    "    Parameters:\n",
    "    - models: Dictionary containing model names as keys and corresponding trained models as values.\n",
    "    - data_split_method: Method used for data splitting.\n",
    "    - target_column: Target column for which models were trained.\n",
    "    - neural_network: Boolean indicating whether the models are neural networks.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Get the current timestamp\n",
    "    now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    # Determine the directory based on the data splitting method and neural network mode\n",
    "    if neural_network:\n",
    "        directory = f'models/{map_name}/neural_network/{target_column}'\n",
    "    elif data_split_method == 'quadrant':\n",
    "        directory = f'models/{map_name}/quadrant/{target_column}'\n",
    "    elif data_split_method == 'slice':\n",
    "        directory = f'models/{map_name}/slice/{target_column}'\n",
    "    else:\n",
    "        directory = f'models/{map_name}/random/{target_column}'\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    create_directory_if_not_exists(directory)\n",
    "\n",
    "    # Save each model to a pickle file with a timestamped filename\n",
    "    for model_name, model in models.items():\n",
    "        model_filename = f'{directory}/{model_name}_{now}.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(model, model_file)\n",
    "\n",
    "        # Print a message indicating the saved model and its location\n",
    "        print(f'Model {model_name} saved at {model_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(X, y, coords, numerical_features, categorical_features, grid, neural_network, data_split_method, part_to_split):\n",
    "    \"\"\"\n",
    "    Execute the machine learning pipeline, including data preprocessing, model training, and evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    - X (pd.DataFrame): Features dataset.\n",
    "    - y (pd.Series): Target variable.\n",
    "    - coords (pd.DataFrame): Coordinates dataset.\n",
    "    - numerical_features (list): List of numerical features.\n",
    "    - categorical_features (list): List of categorical features.\n",
    "    - grid (bool): Flag indicating whether to perform grid search for hyperparameter tuning.\n",
    "    - data_split_method (str): Method for splitting the data ('quadrant', 'slice', or 'random').\n",
    "    - part_to_split (int): Number of parts to split the data.\n",
    "    - neural_network (bool): Flag indicating whether to use a neural network.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Validation scores for each trained model.\n",
    "    - dict: Training metrics for each trained model.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y, coords, data_split_method, part_to_split)\n",
    "\n",
    "    preprocessor = preprocessing_data(numerical_features, categorical_features)\n",
    "\n",
    "    models_pipeline = create_models_pipeline(preprocessor = preprocessor, \n",
    "                                             grid = grid, \n",
    "                                             neural_network = neural_network,\n",
    "                                             input_layer_neurons = 5,#X_train.shape[1],\n",
    "                                             output_layer_neurons = y_train.shape[1], \n",
    "                                             n_folds=10 if data_split_method == 'slice' else 5)\n",
    "    \n",
    "    print(\"Training models\")\n",
    "\n",
    "    trained_models, training_score = train_models(X_train, y_train, models_pipeline, grid)\n",
    "\n",
    "    print(\"Evaluating models\")\n",
    "\n",
    "    validation_score = evaluate_models(X_test, y_test, trained_models, neural_network)\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "    save_models(trained_models, data_split_method, y.name if not neural_network else y.columns.values, neural_network)\n",
    "\n",
    "    # print(training_score)\n",
    "\n",
    "    return validation_score, training_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformando em Slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metric_per_row(df, save_dir, subcolumn='MAPE', title='Column'):\n",
    "    \"\"\"\n",
    "    Save bar charts for a specific metric (subcolumn) for each row in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame with the metrics.\n",
    "    - save_dir (str): Directory to save the bar charts.\n",
    "    - subcolumn (str): Subcolumn within the DataFrame to visualize.\n",
    "    - title (str): Base title for the charts.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Check if the saving directory exists; create it if not\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Get the model names from the DataFrame columns\n",
    "    model_names = df.columns.levels[0]\n",
    "    \n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Create a new directory for each index\n",
    "        index_dir = os.path.join(save_dir, str(index))\n",
    "        os.makedirs(index_dir, exist_ok=True)\n",
    "        \n",
    "        # Get the specified subcolumn values for the current row\n",
    "        subcolumn_values = row.xs(subcolumn, level=1)\n",
    "        \n",
    "        # Sort the values and corresponding model names\n",
    "        sorted_values, sorted_indices = zip(*sorted(zip(subcolumn_values, model_names)))\n",
    "        \n",
    "        # Increase the figure size to ensure enough space for x-axis labels\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create and save the bar chart for the sorted subcolumn values of the current row\n",
    "        plt.bar(np.arange(len(model_names)), sorted_values, width=0.2)\n",
    "        plt.ylabel(subcolumn)\n",
    "        \n",
    "        # Add a customized title (using the index as the title)\n",
    "        title_with_index = f'{title} - {index}'\n",
    "        plt.title(title_with_index)\n",
    "        plt.xticks(np.arange(len(model_names)), sorted_indices, rotation=45, ha='right')\n",
    "        \n",
    "        # Adjust the bottom margin to ensure full visibility of x-axis labels\n",
    "        plt.subplots_adjust(bottom=0.3)\n",
    "        \n",
    "        # Save the figure in the specific directory for the index\n",
    "        file_path = os.path.join(index_dir, f'{index}.png')  # Use the title as the file name\n",
    "        plt.savefig(file_path)\n",
    "        plt.close()\n",
    "\n",
    "def add_simple_slide(prs, target_column, model_name, mse_test_path, mse_training_path, mape_path):\n",
    "    \"\"\"\n",
    "    Adds a slide to a PowerPoint presentation (pptx) containing three graphs: MSE Test, MSE Training, and MAPE\n",
    "    for a specific target column and model.\n",
    "\n",
    "    Parameters:\n",
    "    - prs: PowerPoint presentation (Presentation) where the slide will be added.\n",
    "    - target_column: Name of the target column.\n",
    "    - model_name: Name of the model.\n",
    "    - mse_test_path: Path to the MSE Test graph.\n",
    "    - mse_training_path: Path to the MSE Training graph.\n",
    "    - mape_path: Path to the MAPE graph.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Choose a blank slide layout\n",
    "    blank_slide_layout = prs.slide_layouts[6]\n",
    "\n",
    "    # Add slide with the chosen layout\n",
    "    slide = prs.slides.add_slide(blank_slide_layout)\n",
    "\n",
    "    # Add title\n",
    "    title_text = f\"Analysis for column: {target_column}\\nModel: {model_name}\"\n",
    "    add_text_box(slide, title_text, left=Inches(0), top=Inches(0), width=Inches(10), height=Inches(1))\n",
    "\n",
    "    # Add graphs\n",
    "    add_graph(slide, mse_test_path, top=Inches(2), left=Inches(0.1))\n",
    "    add_graph(slide, mse_training_path, top=Inches(2), left=Inches(3))\n",
    "    add_graph(slide, mape_path, top=Inches(2), left=Inches(6))\n",
    "\n",
    "def add_text_box(slide, text, left, top, width, height):\n",
    "    \"\"\"Adds a text box to a slide.\"\"\"\n",
    "    txBox = slide.shapes.add_textbox(left=left, top=top, width=width, height=height)\n",
    "    text_frame = txBox.text_frame\n",
    "    p = text_frame.add_paragraph()\n",
    "    p.text = text\n",
    "    p.font.size = Pt(18)\n",
    "    p.font.bold = True\n",
    "\n",
    "def add_graph(slide, image_path, top, left, width=Inches(3.2), height=Inches(3.2)):\n",
    "    \"\"\"Adds a graph to a slide.\"\"\"\n",
    "    slide.shapes.add_picture(image_path, left, top, width=width, height=height)\n",
    "\n",
    "def create_simple_slides(mse_test_images_dirs, mse_training_images_dirs, mape_images_dirs, target_columns, output_pptx):\n",
    "    \"\"\"\n",
    "    Creates slides in a PowerPoint presentation (pptx) for the analysis of MSE Test, MSE Training, and MAPE\n",
    "    for specific target columns and models.\n",
    "\n",
    "    Parameters:\n",
    "    - mse_test_images_dirs: Dictionary with model names as keys and directories containing MSE Test images as values.\n",
    "    - mse_training_images_dirs: Dictionary with model names as keys and directories containing MSE Training images as values.\n",
    "    - mape_images_dirs: Dictionary with model names as keys and directories containing MAPE images as values.\n",
    "    - target_columns: List of target columns.\n",
    "    - output_pptx: Path to the output pptx file.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Initialize the presentation\n",
    "    prs = Presentation()\n",
    "\n",
    "    # Add slides for each column and model\n",
    "    for target_column in target_columns:\n",
    "        for model_name in mse_test_images_dirs:  # Assuming mse_test_images_dirs contains only the desired model directories\n",
    "            # Find paths to the images\n",
    "            mse_test_path = os.path.join(mse_test_images_dirs[model_name], target_column, f'{target_column}.png')\n",
    "            mse_training_path = os.path.join(mse_training_images_dirs[model_name], target_column, f'{target_column}.png')\n",
    "            mape_path = os.path.join(mape_images_dirs[model_name], target_column, f'{target_column}.png')\n",
    "\n",
    "            # Add slide with the three images\n",
    "            add_simple_slide(prs, target_column, model_name, mse_test_path, mse_training_path, mape_path)\n",
    "\n",
    "    # Save the presentation\n",
    "    prs.save(output_pptx)\n",
    "\n",
    "\n",
    "def create_files_results(train_scores, test_scores, target_columns, neural_network=False):\n",
    "    \"\"\"\n",
    "    Create image files and a PowerPoint presentation from training and test scores.\n",
    "\n",
    "    Parameters:\n",
    "    - train_scores (pd.DataFrame): DataFrame containing training scores.\n",
    "    - test_scores (pd.DataFrame): DataFrame containing test scores.\n",
    "    - target_columns (list): List of target columns.\n",
    "    - neural_network (bool): Flag indicating whether the model is a neural network. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the base directory for saving images and slides\n",
    "    base_dir = f'images/{map_name}/metrics'\n",
    "    if neural_network:\n",
    "        base_dir = f'{base_dir}/neural_network'\n",
    "    else:\n",
    "        base_dir = f'{base_dir}/all_models'\n",
    "\n",
    "    # Define directories for saving images\n",
    "    test_metrics_dir = f'{base_dir}/test_metrics'\n",
    "    training_metrics_dir = f'{base_dir}/training_metrics'\n",
    "\n",
    "    test_mse_dir = f'{test_metrics_dir}/mse'\n",
    "    test_mape_dir = f'{test_metrics_dir}/mape'\n",
    "    test_mae_dir = f'{test_metrics_dir}/mae'\n",
    "\n",
    "    train_mean_mse_dir = f'{training_metrics_dir}/mean'\n",
    "    train_std_mse_dir = f'{training_metrics_dir}/std'\n",
    "\n",
    "    # Create images results from TEST SCORES\n",
    "    save_metric_per_row(test_scores, save_dir=test_mape_dir, subcolumn='MAPE', title='Test Scores - MAPE')\n",
    "    save_metric_per_row(test_scores, save_dir=test_mse_dir, subcolumn='MSE', title='Test Scores - MSE')\n",
    "    save_metric_per_row(test_scores, save_dir=test_mae_dir, subcolumn='MAE', title='Test Scores - MAE')\n",
    "\n",
    "    # Create images results from TRAINING SCORES if not a neural network\n",
    "    if not neural_network:\n",
    "        save_metric_per_row(train_scores, save_dir=train_mean_mse_dir, subcolumn='Mean MSE', title='Training Scores - Mean MSE')\n",
    "        save_metric_per_row(train_scores, save_dir=train_std_mse_dir, subcolumn='Std MSE', title='Training Scores - Std MSE')\n",
    "\n",
    "\n",
    "    middle_metric_dir = f'{train_mean_mse_dir}' if not neural_network else f'{test_mae_dir}'\n",
    "\n",
    "    # Define directories for MSE images\n",
    "    mse_test_images_dirs = {'All Models': test_mse_dir}\n",
    "\n",
    "    middle_metric_dirs = {'All Models': middle_metric_dir}\n",
    "\n",
    "    # Define directory for MAPE images\n",
    "    mape_images_dirs = {'All Models': test_mape_dir}\n",
    "\n",
    "    # Define directory for MAE images\n",
    "    mae_images_dirs = {'All Models': test_mae_dir}\n",
    "\n",
    "    # Create PowerPoint presentation with the results\n",
    "    slides_dir = f'slides/{map_name}'\n",
    "\n",
    "    create_simple_slides(mse_test_images_dirs, middle_metric_dirs, mape_images_dirs, target_columns,\n",
    "                         f'{slides_dir}/map2_all_models_metrics_results.pptx' if not neural_network else f'{slides_dir}/map2_neural_network_metrics_results.pptx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executando pipeline por coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_df(results_dict, metrics):\n",
    "    \"\"\"\n",
    "    Creates a final DataFrame by consolidating results for multiple targets and models.\n",
    "\n",
    "    Parameters:\n",
    "    - results_dict: Dictionary containing results for different targets and models.\n",
    "    - metrics: List of performance metrics.\n",
    "\n",
    "    Returns:\n",
    "    - final_df: Consolidated DataFrame with results.\n",
    "    \"\"\"\n",
    "    # Convert the dictionary to a DataFrame with models as columns and targets as rows\n",
    "    consolidated_df = pd.DataFrame(results_dict).T\n",
    "\n",
    "    # Sort columns alphabetically for better readability\n",
    "    consolidated_df = consolidated_df.sort_index(axis=1)\n",
    "\n",
    "    # Initialize an empty DataFrame for the final results\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each model and its corresponding results\n",
    "    for model_name, model_results in consolidated_df.items():\n",
    "        # Convert the model's results to a DataFrame with metrics as columns\n",
    "        model_df = pd.DataFrame(model_results.values.tolist(), index=model_results.index, columns=metrics)\n",
    "\n",
    "        # Create a MultiIndex for columns with the model's name and metric names\n",
    "        model_df.columns = pd.MultiIndex.from_product([[model_name], model_df.columns])\n",
    "\n",
    "        # Concatenate the model's DataFrame to the final DataFrame\n",
    "        final_df = pd.concat([final_df, model_df], axis=1)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def process_multiple_targets_pipeline(filepath, grid=False, neural_network=False, data_split_method='random', part_to_split=10):\n",
    "    \"\"\"\n",
    "    Processes a pipeline for multiple target columns and models, including training and evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath: Path to the dataset file.\n",
    "    - grid: Boolean indicating whether to perform a grid search for hyperparameter tuning.\n",
    "    - data_split_method: Method for splitting the data (default: 'random').\n",
    "    - part_to_split: Number of parts to split the data if using 'random' split method.\n",
    "\n",
    "    Returns:\n",
    "    - final_df: Consolidated DataFrame with evaluation results.\n",
    "    - training_scores_df: DataFrame with training scores (Mean MSE and Std MSE).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data from the specified file\n",
    "    data = load_data(filepath)\n",
    "\n",
    "    # Extract coordinates from the data\n",
    "    coords = data[['latitude', 'longitude']]\n",
    "\n",
    "    # Filter variables to separate bands and nutrients\n",
    "    bands, nutrients = filter_variables_by_name(data, exclude_coordinates=True)\n",
    "\n",
    "    target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'B', 'Cu', 'Mn', 'Zn', 'Mo', 'Ni']\n",
    "\n",
    "    if neural_network:\n",
    "        print(\"Neural Network Mode\")\n",
    "\n",
    "        # Train and evaluate models for all target nutrients using the pipeline\n",
    "        test_scores, train_scores = pipeline(bands, nutrients[target_columns], coords, bands.columns, [], grid, neural_network, data_split_method, part_to_split)\n",
    "\n",
    "        # Save the results\n",
    "        save_results(df=test_scores, data_split_method=data_split_method, part_to_split=part_to_split, grid=grid, neural_network=neural_network)\n",
    "        create_files_results(train_scores=train_scores, test_scores=test_scores, target_columns=target_columns, neural_network=True)\n",
    "\n",
    "        return test_scores, train_scores\n",
    "    else:\n",
    "        test_dict = {}\n",
    "        train_dict = {}\n",
    "\n",
    "        # Iterate over each target nutrient\n",
    "        for nutrient_name, nutrient_values in nutrients.items():\n",
    "            print(f'Processing {nutrient_name}')\n",
    "\n",
    "            # Train and evaluate models for the target nutrient using the pipeline\n",
    "            test_scores, train_scores = pipeline(bands, nutrient_values, coords, bands.columns, [], grid, neural_network, data_split_method, part_to_split)\n",
    "\n",
    "            # Add results to the dictionaries\n",
    "            test_dict[nutrient_name] = test_scores\n",
    "            train_dict[nutrient_name] = train_scores\n",
    "\n",
    "        # Create a consolidated DataFrame with the evaluation results for test data\n",
    "        final_test_scores = create_final_df(test_dict, metrics=[\"MSE\", \"MAE\", \"R2\", \"MAPE\"])\n",
    "\n",
    "        # Create a consolidated DataFrame with the evaluation results for train data\n",
    "        final_train_scores = create_final_df(train_dict, metrics=[\"Mean MSE\", \"Std MSE\"])\n",
    "\n",
    "        # Save the results\n",
    "        save_results(final_test_scores, data_split_method, part_to_split, grid, neural_network)\n",
    "        create_files_results(final_train_scores, final_test_scores, target_columns)\n",
    "\n",
    "        return final_test_scores, final_train_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste do Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N\n",
      "Training models\n",
      "Training LinearRegression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -36.368761925828956\n",
      "\n",
      "Mean CVs score: -36.368761925828956\n",
      "Stds CVs score: 14.231244368702678\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/N/LinearRegression_2024-02-26_20-50-36.pkl\n",
      "Processing P\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.18116807148614697\n",
      "\n",
      "Mean CVs score: -0.18116807148614697\n",
      "Stds CVs score: 0.06607053671532523\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/P/LinearRegression_2024-02-26_20-50-36.pkl\n",
      "Processing K\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -2.8796251104957387\n",
      "\n",
      "Mean CVs score: -2.8796251104957387\n",
      "Stds CVs score: 1.8709924239598905\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/K/LinearRegression_2024-02-26_20-50-38.pkl\n",
      "Processing Ca\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.1542105785886119\n",
      "\n",
      "Mean CVs score: -0.1542105785886119\n",
      "Stds CVs score: 0.06910730118361565\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Ca/LinearRegression_2024-02-26_20-50-39.pkl\n",
      "Processing Mg\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.014025454326570746\n",
      "\n",
      "Mean CVs score: -0.014025454326570746\n",
      "Stds CVs score: 0.002432662782399168\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Mg/LinearRegression_2024-02-26_20-50-40.pkl\n",
      "Processing S\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.013068193854753597\n",
      "\n",
      "Mean CVs score: -0.013068193854753597\n",
      "Stds CVs score: 0.002529351746633083\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/S/LinearRegression_2024-02-26_20-50-41.pkl\n",
      "Processing B\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -4.653175249668524\n",
      "\n",
      "Mean CVs score: -4.653175249668524\n",
      "Stds CVs score: 2.236405075458448\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/B/LinearRegression_2024-02-26_20-50-42.pkl\n",
      "Processing Cu\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -1.461133648092617\n",
      "\n",
      "Mean CVs score: -1.461133648092617\n",
      "Stds CVs score: 0.9128511485557106\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Cu/LinearRegression_2024-02-26_20-50-42.pkl\n",
      "Processing Fe\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -4795.732512577628\n",
      "\n",
      "Mean CVs score: -4795.732512577628\n",
      "Stds CVs score: 3557.577953981092\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Fe/LinearRegression_2024-02-26_20-50-43.pkl\n",
      "Processing Mn\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -63.451547892555006\n",
      "\n",
      "Mean CVs score: -63.451547892555006\n",
      "Stds CVs score: 24.056617565146336\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Mn/LinearRegression_2024-02-26_20-50-44.pkl\n",
      "Processing Zn\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -6.26105406333079\n",
      "\n",
      "Mean CVs score: -6.26105406333079\n",
      "Stds CVs score: 2.203791301891954\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Zn/LinearRegression_2024-02-26_20-50-45.pkl\n",
      "Processing Mo\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.2002809912807239\n",
      "\n",
      "Mean CVs score: -0.2002809912807239\n",
      "Stds CVs score: 0.08825325812828437\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Mo/LinearRegression_2024-02-26_20-50-45.pkl\n",
      "Processing Ni\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.06866527169210129\n",
      "\n",
      "Mean CVs score: -0.06866527169210129\n",
      "Stds CVs score: 0.025268787855314015\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Ni/LinearRegression_2024-02-26_20-50-46.pkl\n",
      "Processing Al\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -1670.3232553639332\n",
      "\n",
      "Mean CVs score: -1670.3232553639332\n",
      "Stds CVs score: 1202.1940514731455\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Al/LinearRegression_2024-02-26_20-50-47.pkl\n",
      "Processing Se\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.49699287254003754\n",
      "\n",
      "Mean CVs score: -0.49699287254003754\n",
      "Stds CVs score: 0.35318954240262634\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Se/LinearRegression_2024-02-26_20-50-49.pkl\n",
      "Processing Si\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -21926.366754022525\n",
      "\n",
      "Mean CVs score: -21926.366754022525\n",
      "Stds CVs score: 13479.42847263\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Si/LinearRegression_2024-02-26_20-50-50.pkl\n",
      "Processing Na\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -46.47670410475247\n",
      "\n",
      "Mean CVs score: -46.47670410475247\n",
      "Stds CVs score: 32.38349982522598\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Na/LinearRegression_2024-02-26_20-50-51.pkl\n",
      "Processing Va\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.02769076762072888\n",
      "\n",
      "Mean CVs score: -0.02769076762072888\n",
      "Stds CVs score: 0.018675853943774575\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/Va/LinearRegression_2024-02-26_20-50-52.pkl\n",
      "Processing DRIS_N\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.11647432865170766\n",
      "\n",
      "Mean CVs score: -0.11647432865170766\n",
      "Stds CVs score: 0.07011840102518929\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/DRIS_N/LinearRegression_2024-02-26_20-50-53.pkl\n",
      "Processing DRIS_P\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.378995013026344\n",
      "\n",
      "Mean CVs score: -0.378995013026344\n",
      "Stds CVs score: 0.17361387025702685\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/DRIS_P/LinearRegression_2024-02-26_20-50-53.pkl\n",
      "Processing DRIS_K\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.6701953924174917\n",
      "\n",
      "Mean CVs score: -0.6701953924174917\n",
      "Stds CVs score: 0.8566446551133023\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/DRIS_K/LinearRegression_2024-02-26_20-50-54.pkl\n",
      "Processing DRIS_Ca\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.12163062237635144\n",
      "\n",
      "Mean CVs score: -0.12163062237635144\n",
      "Stds CVs score: 0.09415999947435306\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/DRIS_Ca/LinearRegression_2024-02-26_20-50-55.pkl\n",
      "Processing DRIS_Mg\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.2754053672299192\n",
      "\n",
      "Mean CVs score: -0.2754053672299192\n",
      "Stds CVs score: 0.34347331476820014\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/DRIS_Mg/LinearRegression_2024-02-26_20-50-56.pkl\n",
      "Processing DRIS_S\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.5281452408264059\n",
      "\n",
      "Mean CVs score: -0.5281452408264059\n",
      "Stds CVs score: 0.2996398560092697\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/DRIS_S/LinearRegression_2024-02-26_20-50-56.pkl\n",
      "Processing DRIS_B\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.11731404694711471\n",
      "\n",
      "Mean CVs score: -0.11731404694711471\n",
      "Stds CVs score: 0.04871469373110586\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/DRIS_B/LinearRegression_2024-02-26_20-50-57.pkl\n",
      "Processing DRIS_Cu\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.4503206162638578\n",
      "\n",
      "Mean CVs score: -0.4503206162638578\n",
      "Stds CVs score: 0.4459428294158648\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/DRIS_Cu/LinearRegression_2024-02-26_20-50-58.pkl\n",
      "Processing DRIS_Fe\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -6.059886336529779\n",
      "\n",
      "Mean CVs score: -6.059886336529779\n",
      "Stds CVs score: 4.6303547384315396\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/DRIS_Fe/LinearRegression_2024-02-26_20-50-58.pkl\n",
      "Processing DRIS_Mn\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.32870636587526614\n",
      "\n",
      "Mean CVs score: -0.32870636587526614\n",
      "Stds CVs score: 0.45922972471872286\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/DRIS_Mn/LinearRegression_2024-02-26_20-50-59.pkl\n",
      "Processing DRIS_Zn\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.510969020941417\n",
      "\n",
      "Mean CVs score: -0.510969020941417\n",
      "Stds CVs score: 0.8474382834414105\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/DRIS_Zn/LinearRegression_2024-02-26_20-51-00.pkl\n",
      "Processing IMS\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -0.17775335641169035\n",
      "\n",
      "Mean CVs score: -0.17775335641169035\n",
      "Stds CVs score: 0.23955094329390822\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/IMS/LinearRegression_2024-02-26_20-51-01.pkl\n",
      "Processing IBN\n",
      "Training models\n",
      "Training LinearRegression\n",
      "Best parameters of LinearRegression are: {'dim_red__n_components': 5}\n",
      "Best score of LinearRegression is: -13.751523416460973\n",
      "\n",
      "Mean CVs score: -13.751523416460973\n",
      "Stds CVs score: 14.667629363483584\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "Model LinearRegression saved at models/map2/slice/IBN/LinearRegression_2024-02-26_20-51-01.pkl\n",
      "Results saved at data/map2/prediction_results/all_models/all_models_2024-02-26_20-51-01_part_splited_5_results.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">LinearRegression</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>7.846987</td>\n",
       "      <td>2.171275</td>\n",
       "      <td>-0.073502</td>\n",
       "      <td>0.046691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.053620</td>\n",
       "      <td>0.193018</td>\n",
       "      <td>-1.886341</td>\n",
       "      <td>0.038095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>2.117567</td>\n",
       "      <td>1.244338</td>\n",
       "      <td>-0.112259</td>\n",
       "      <td>0.078454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ca</th>\n",
       "      <td>0.162905</td>\n",
       "      <td>0.359027</td>\n",
       "      <td>0.196101</td>\n",
       "      <td>0.046494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mg</th>\n",
       "      <td>0.009885</td>\n",
       "      <td>0.083397</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.031948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.008937</td>\n",
       "      <td>0.090046</td>\n",
       "      <td>0.181476</td>\n",
       "      <td>0.049586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>5.230432</td>\n",
       "      <td>1.964877</td>\n",
       "      <td>0.146227</td>\n",
       "      <td>0.061521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cu</th>\n",
       "      <td>0.168328</td>\n",
       "      <td>0.326173</td>\n",
       "      <td>-0.873944</td>\n",
       "      <td>0.037604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe</th>\n",
       "      <td>1237.785482</td>\n",
       "      <td>32.330660</td>\n",
       "      <td>-31.095491</td>\n",
       "      <td>0.356649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mn</th>\n",
       "      <td>33.007310</td>\n",
       "      <td>3.729012</td>\n",
       "      <td>-0.370195</td>\n",
       "      <td>0.043073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zn</th>\n",
       "      <td>1.878042</td>\n",
       "      <td>1.021392</td>\n",
       "      <td>-0.314196</td>\n",
       "      <td>0.025155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo</th>\n",
       "      <td>0.145119</td>\n",
       "      <td>0.328671</td>\n",
       "      <td>-0.044458</td>\n",
       "      <td>0.344111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ni</th>\n",
       "      <td>0.098936</td>\n",
       "      <td>0.285389</td>\n",
       "      <td>-0.034946</td>\n",
       "      <td>0.172013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Al</th>\n",
       "      <td>500.453480</td>\n",
       "      <td>19.950799</td>\n",
       "      <td>-17.663002</td>\n",
       "      <td>0.213056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Se</th>\n",
       "      <td>0.299516</td>\n",
       "      <td>0.439183</td>\n",
       "      <td>0.172267</td>\n",
       "      <td>0.119997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Si</th>\n",
       "      <td>7244.185800</td>\n",
       "      <td>71.270568</td>\n",
       "      <td>0.129722</td>\n",
       "      <td>0.067168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Na</th>\n",
       "      <td>21.654577</td>\n",
       "      <td>3.617777</td>\n",
       "      <td>-3.792717</td>\n",
       "      <td>0.300568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Va</th>\n",
       "      <td>0.009481</td>\n",
       "      <td>0.085773</td>\n",
       "      <td>-11.788359</td>\n",
       "      <td>0.730510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIS_N</th>\n",
       "      <td>0.034941</td>\n",
       "      <td>0.152105</td>\n",
       "      <td>-0.302388</td>\n",
       "      <td>1.536869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIS_P</th>\n",
       "      <td>0.043602</td>\n",
       "      <td>0.152772</td>\n",
       "      <td>-1.557492</td>\n",
       "      <td>0.075952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIS_K</th>\n",
       "      <td>0.148834</td>\n",
       "      <td>0.321050</td>\n",
       "      <td>0.117397</td>\n",
       "      <td>1.097708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIS_Ca</th>\n",
       "      <td>0.070278</td>\n",
       "      <td>0.222516</td>\n",
       "      <td>-0.277130</td>\n",
       "      <td>5.266996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIS_Mg</th>\n",
       "      <td>0.041546</td>\n",
       "      <td>0.164752</td>\n",
       "      <td>0.056570</td>\n",
       "      <td>3.606287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIS_S</th>\n",
       "      <td>0.138078</td>\n",
       "      <td>0.306523</td>\n",
       "      <td>-1.776917</td>\n",
       "      <td>0.329902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIS_B</th>\n",
       "      <td>0.133114</td>\n",
       "      <td>0.329744</td>\n",
       "      <td>-0.339688</td>\n",
       "      <td>4.442314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIS_Cu</th>\n",
       "      <td>0.065154</td>\n",
       "      <td>0.197798</td>\n",
       "      <td>-1.057182</td>\n",
       "      <td>0.079794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIS_Fe</th>\n",
       "      <td>1.408369</td>\n",
       "      <td>1.078320</td>\n",
       "      <td>-29.485988</td>\n",
       "      <td>1.696622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIS_Mn</th>\n",
       "      <td>0.037345</td>\n",
       "      <td>0.116311</td>\n",
       "      <td>-0.069847</td>\n",
       "      <td>0.082050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRIS_Zn</th>\n",
       "      <td>0.036403</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>-2.644688</td>\n",
       "      <td>0.219285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMS</th>\n",
       "      <td>0.014082</td>\n",
       "      <td>0.088527</td>\n",
       "      <td>-24.223693</td>\n",
       "      <td>0.170369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBN</th>\n",
       "      <td>4.251490</td>\n",
       "      <td>1.922013</td>\n",
       "      <td>-8.513940</td>\n",
       "      <td>0.184392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        LinearRegression                                \n",
       "                     MSE        MAE         R2      MAPE\n",
       "N               7.846987   2.171275  -0.073502  0.046691\n",
       "P               0.053620   0.193018  -1.886341  0.038095\n",
       "K               2.117567   1.244338  -0.112259  0.078454\n",
       "Ca              0.162905   0.359027   0.196101  0.046494\n",
       "Mg              0.009885   0.083397   0.000345  0.031948\n",
       "S               0.008937   0.090046   0.181476  0.049586\n",
       "B               5.230432   1.964877   0.146227  0.061521\n",
       "Cu              0.168328   0.326173  -0.873944  0.037604\n",
       "Fe           1237.785482  32.330660 -31.095491  0.356649\n",
       "Mn             33.007310   3.729012  -0.370195  0.043073\n",
       "Zn              1.878042   1.021392  -0.314196  0.025155\n",
       "Mo              0.145119   0.328671  -0.044458  0.344111\n",
       "Ni              0.098936   0.285389  -0.034946  0.172013\n",
       "Al            500.453480  19.950799 -17.663002  0.213056\n",
       "Se              0.299516   0.439183   0.172267  0.119997\n",
       "Si           7244.185800  71.270568   0.129722  0.067168\n",
       "Na             21.654577   3.617777  -3.792717  0.300568\n",
       "Va              0.009481   0.085773 -11.788359  0.730510\n",
       "DRIS_N          0.034941   0.152105  -0.302388  1.536869\n",
       "DRIS_P          0.043602   0.152772  -1.557492  0.075952\n",
       "DRIS_K          0.148834   0.321050   0.117397  1.097708\n",
       "DRIS_Ca         0.070278   0.222516  -0.277130  5.266996\n",
       "DRIS_Mg         0.041546   0.164752   0.056570  3.606287\n",
       "DRIS_S          0.138078   0.306523  -1.776917  0.329902\n",
       "DRIS_B          0.133114   0.329744  -0.339688  4.442314\n",
       "DRIS_Cu         0.065154   0.197798  -1.057182  0.079794\n",
       "DRIS_Fe         1.408369   1.078320 -29.485988  1.696622\n",
       "DRIS_Mn         0.037345   0.116311  -0.069847  0.082050\n",
       "DRIS_Zn         0.036403   0.147200  -2.644688  0.219285\n",
       "IMS             0.014082   0.088527 -24.223693  0.170369\n",
       "IBN             4.251490   1.922013  -8.513940  0.184392"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_path = f'data/{map_name}/interpolation/universal_interpolated_df.csv'\n",
    "file_path = f'data/{map_name}/interpolation/nearest_neighbors_interpolation_df.csv'\n",
    "\n",
    "\n",
    "test_scores, train_scores = process_multiple_targets_pipeline(file_path, grid=True, data_split_method='slice', neural_network = False, part_to_split=5)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Mode\n",
      "Training models\n",
      "Training NeuralNetwork\n",
      "Best parameters of NeuralNetwork are: {'model__batch_size': 32, 'model__epochs': 100, 'model__input_neurons': 5, 'model__loss': 'mse', 'model__optimizer': 'adam', 'model__output_neurons': 12}\n",
      "Best score of NeuralNetwork is: -9.786057050517439\n",
      "\n",
      "Mean CVs score: -9.786057050517439\n",
      "Stds CVs score: 2.090294836745772\n",
      "\n",
      "\n",
      "Evaluating models\n",
      "Done\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Lucas\\AppData\\Local\\Temp\\tmp6ub3gfta\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Lucas\\AppData\\Local\\Temp\\tmp6ub3gfta\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model NeuralNetwork saved at models/map2/neural_network/['N' 'P' 'K' 'Ca' 'Mg' 'S' 'B' 'Cu' 'Mn' 'Zn' 'Mo' 'Ni']/NeuralNetwork_2024-02-26_21-02-00.pkl\n",
      "Results saved at data/map2/prediction_results/neural_network/neural_network_2024-02-26_21-02-01_part_splited_5_results.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">NeuralNetwork</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>8.319985</td>\n",
       "      <td>2.178986</td>\n",
       "      <td>-0.138210</td>\n",
       "      <td>0.046714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.036268</td>\n",
       "      <td>0.151055</td>\n",
       "      <td>-0.952306</td>\n",
       "      <td>0.029686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>2.366067</td>\n",
       "      <td>1.308656</td>\n",
       "      <td>-0.242785</td>\n",
       "      <td>0.082943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ca</th>\n",
       "      <td>0.175959</td>\n",
       "      <td>0.365230</td>\n",
       "      <td>0.131679</td>\n",
       "      <td>0.047154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mg</th>\n",
       "      <td>0.009985</td>\n",
       "      <td>0.082274</td>\n",
       "      <td>-0.009718</td>\n",
       "      <td>0.031686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.009071</td>\n",
       "      <td>0.089077</td>\n",
       "      <td>0.169213</td>\n",
       "      <td>0.049812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>6.543761</td>\n",
       "      <td>2.102554</td>\n",
       "      <td>-0.068150</td>\n",
       "      <td>0.065207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cu</th>\n",
       "      <td>0.204683</td>\n",
       "      <td>0.365007</td>\n",
       "      <td>-1.278675</td>\n",
       "      <td>0.042103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mn</th>\n",
       "      <td>52.217254</td>\n",
       "      <td>4.878721</td>\n",
       "      <td>-1.167636</td>\n",
       "      <td>0.056087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zn</th>\n",
       "      <td>3.424264</td>\n",
       "      <td>1.291132</td>\n",
       "      <td>-1.396195</td>\n",
       "      <td>0.031835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mo</th>\n",
       "      <td>0.133015</td>\n",
       "      <td>0.303577</td>\n",
       "      <td>0.042654</td>\n",
       "      <td>0.309304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ni</th>\n",
       "      <td>0.105579</td>\n",
       "      <td>0.287687</td>\n",
       "      <td>-0.104432</td>\n",
       "      <td>0.171116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NeuralNetwork                              \n",
       "             MSE       MAE        R2      MAPE\n",
       "N       8.319985  2.178986 -0.138210  0.046714\n",
       "P       0.036268  0.151055 -0.952306  0.029686\n",
       "K       2.366067  1.308656 -0.242785  0.082943\n",
       "Ca      0.175959  0.365230  0.131679  0.047154\n",
       "Mg      0.009985  0.082274 -0.009718  0.031686\n",
       "S       0.009071  0.089077  0.169213  0.049812\n",
       "B       6.543761  2.102554 -0.068150  0.065207\n",
       "Cu      0.204683  0.365007 -1.278675  0.042103\n",
       "Mn     52.217254  4.878721 -1.167636  0.056087\n",
       "Zn      3.424264  1.291132 -1.396195  0.031835\n",
       "Mo      0.133015  0.303577  0.042654  0.309304\n",
       "Ni      0.105579  0.287687 -0.104432  0.171116"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_path = f'data/{map_name}/interpolation/universal_interpolated_df.csv'\n",
    "file_path = f'data/{map_name}/interpolation/nearest_neighbors_interpolation_df.csv'\n",
    "\n",
    "test_scores, train_scores = process_multiple_targets_pipeline(file_path, grid=True, neural_network = True, data_split_method='slice', part_to_split=5)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NeuralNetwork': {'Mean MSE': 9.786057050517439,\n",
       "  'Std MSE': 2.090294836745772}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
